<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_z4hqe1y6t2ui-0>li:before{content:"\0025cf   "}.lst-kix_z4hqe1y6t2ui-2>li:before{content:"\0025a0   "}.lst-kix_z4hqe1y6t2ui-3>li:before{content:"\0025cf   "}.lst-kix_blglcosvosdw-1>li:before{content:"\0025cb   "}.lst-kix_z4hqe1y6t2ui-4>li:before{content:"\0025cb   "}.lst-kix_z4hqe1y6t2ui-5>li:before{content:"\0025a0   "}.lst-kix_blglcosvosdw-0>li:before{content:"\0025cf   "}.lst-kix_blglcosvosdw-5>li:before{content:"\0025a0   "}ul.lst-kix_lvvmpwix8sn0-4{list-style-type:none}ul.lst-kix_lvvmpwix8sn0-3{list-style-type:none}ul.lst-kix_lvvmpwix8sn0-2{list-style-type:none}ul.lst-kix_lvvmpwix8sn0-1{list-style-type:none}ul.lst-kix_lvvmpwix8sn0-0{list-style-type:none}ul.lst-kix_z4hqe1y6t2ui-0{list-style-type:none}.lst-kix_blglcosvosdw-2>li:before{content:"\0025a0   "}.lst-kix_blglcosvosdw-6>li:before{content:"\0025cf   "}ul.lst-kix_z4hqe1y6t2ui-1{list-style-type:none}ul.lst-kix_z4hqe1y6t2ui-2{list-style-type:none}.lst-kix_blglcosvosdw-3>li:before{content:"\0025cf   "}ul.lst-kix_lvvmpwix8sn0-8{list-style-type:none}ul.lst-kix_lvvmpwix8sn0-7{list-style-type:none}.lst-kix_blglcosvosdw-4>li:before{content:"\0025cb   "}.lst-kix_z4hqe1y6t2ui-1>li:before{content:"\0025cb   "}ul.lst-kix_lvvmpwix8sn0-6{list-style-type:none}ul.lst-kix_lvvmpwix8sn0-5{list-style-type:none}.lst-kix_6q5g7igxj1tk-2>li:before{content:"\0025a0   "}ul.lst-kix_6q5g7igxj1tk-5{list-style-type:none}ul.lst-kix_6q5g7igxj1tk-6{list-style-type:none}.lst-kix_6q5g7igxj1tk-3>li:before{content:"\0025cf   "}ul.lst-kix_6q5g7igxj1tk-3{list-style-type:none}ul.lst-kix_6q5g7igxj1tk-4{list-style-type:none}.lst-kix_6q5g7igxj1tk-4>li:before{content:"\0025cb   "}ul.lst-kix_6q5g7igxj1tk-7{list-style-type:none}.lst-kix_5izwj51keqm7-4>li:before{content:"-  "}ul.lst-kix_6q5g7igxj1tk-8{list-style-type:none}ul.lst-kix_2e2fmmrc38gz-2{list-style-type:none}.lst-kix_6q5g7igxj1tk-6>li:before{content:"\0025cf   "}ul.lst-kix_2e2fmmrc38gz-1{list-style-type:none}ul.lst-kix_2e2fmmrc38gz-4{list-style-type:none}.lst-kix_6q5g7igxj1tk-5>li:before{content:"\0025a0   "}.lst-kix_6q5g7igxj1tk-7>li:before{content:"\0025cb   "}.lst-kix_5izwj51keqm7-5>li:before{content:"-  "}ul.lst-kix_2e2fmmrc38gz-3{list-style-type:none}ul.lst-kix_2e2fmmrc38gz-6{list-style-type:none}ul.lst-kix_2e2fmmrc38gz-5{list-style-type:none}ul.lst-kix_2e2fmmrc38gz-8{list-style-type:none}.lst-kix_5izwj51keqm7-6>li:before{content:"-  "}ul.lst-kix_2e2fmmrc38gz-7{list-style-type:none}.lst-kix_z4hqe1y6t2ui-7>li:before{content:"\0025cb   "}.lst-kix_z4hqe1y6t2ui-8>li:before{content:"\0025a0   "}.lst-kix_5izwj51keqm7-7>li:before{content:"-  "}.lst-kix_6q5g7igxj1tk-8>li:before{content:"\0025a0   "}.lst-kix_z4hqe1y6t2ui-6>li:before{content:"\0025cf   "}ul.lst-kix_2e2fmmrc38gz-0{list-style-type:none}.lst-kix_5izwj51keqm7-8>li:before{content:"-  "}ul.lst-kix_g2z3zcyhlk66-1{list-style-type:none}ul.lst-kix_g2z3zcyhlk66-0{list-style-type:none}ul.lst-kix_g2z3zcyhlk66-3{list-style-type:none}ul.lst-kix_6q5g7igxj1tk-1{list-style-type:none}ul.lst-kix_g2z3zcyhlk66-2{list-style-type:none}ul.lst-kix_6q5g7igxj1tk-2{list-style-type:none}ul.lst-kix_g2z3zcyhlk66-5{list-style-type:none}ul.lst-kix_g2z3zcyhlk66-4{list-style-type:none}ul.lst-kix_6q5g7igxj1tk-0{list-style-type:none}.lst-kix_lvvmpwix8sn0-1>li:before{content:"\0025cb   "}ul.lst-kix_wu4ejk04ds8-1{list-style-type:none}ul.lst-kix_wu4ejk04ds8-0{list-style-type:none}.lst-kix_lvvmpwix8sn0-0>li:before{content:"\0025cf   "}.lst-kix_2e2fmmrc38gz-0>li:before{content:"\0025cf   "}ul.lst-kix_wu4ejk04ds8-3{list-style-type:none}ul.lst-kix_wu4ejk04ds8-2{list-style-type:none}.lst-kix_lvvmpwix8sn0-5>li:before{content:"\0025a0   "}.lst-kix_2e2fmmrc38gz-1>li:before{content:"\0025cb   "}.lst-kix_lvvmpwix8sn0-3>li:before{content:"\0025cf   "}.lst-kix_lvvmpwix8sn0-7>li:before{content:"\0025cb   "}.lst-kix_lvvmpwix8sn0-2>li:before{content:"\0025a0   "}.lst-kix_lvvmpwix8sn0-6>li:before{content:"\0025cf   "}.lst-kix_2e2fmmrc38gz-2>li:before{content:"\0025a0   "}.lst-kix_5izwj51keqm7-3>li:before{content:"-  "}.lst-kix_2e2fmmrc38gz-3>li:before{content:"\0025cf   "}.lst-kix_2e2fmmrc38gz-5>li:before{content:"\0025a0   "}.lst-kix_lvvmpwix8sn0-4>li:before{content:"\0025cb   "}.lst-kix_5izwj51keqm7-2>li:before{content:"-  "}.lst-kix_2e2fmmrc38gz-4>li:before{content:"\0025cb   "}.lst-kix_6q5g7igxj1tk-1>li:before{content:"\0025cb   "}.lst-kix_5izwj51keqm7-1>li:before{content:"-  "}.lst-kix_2e2fmmrc38gz-7>li:before{content:"\0025cb   "}.lst-kix_6q5g7igxj1tk-0>li:before{content:"\0025cf   "}.lst-kix_g2z3zcyhlk66-0>li:before{content:"\0025cf   "}.lst-kix_5izwj51keqm7-0>li:before{content:"-  "}.lst-kix_2e2fmmrc38gz-6>li:before{content:"\0025cf   "}.lst-kix_blglcosvosdw-8>li:before{content:"\0025a0   "}.lst-kix_g2z3zcyhlk66-5>li:before{content:"\0025a0   "}.lst-kix_blglcosvosdw-7>li:before{content:"\0025cb   "}.lst-kix_g2z3zcyhlk66-6>li:before{content:"\0025cf   "}.lst-kix_2e2fmmrc38gz-8>li:before{content:"\0025a0   "}ul.lst-kix_wu4ejk04ds8-5{list-style-type:none}ul.lst-kix_wu4ejk04ds8-4{list-style-type:none}ul.lst-kix_wu4ejk04ds8-7{list-style-type:none}ul.lst-kix_wu4ejk04ds8-6{list-style-type:none}ul.lst-kix_wu4ejk04ds8-8{list-style-type:none}.lst-kix_g2z3zcyhlk66-4>li:before{content:"\0025cb   "}.lst-kix_g2z3zcyhlk66-3>li:before{content:"\0025cf   "}.lst-kix_g2z3zcyhlk66-1>li:before{content:"\0025cb   "}.lst-kix_g2z3zcyhlk66-2>li:before{content:"\0025a0   "}.lst-kix_g2z3zcyhlk66-7>li:before{content:"\0025cb   "}.lst-kix_g2z3zcyhlk66-8>li:before{content:"\0025a0   "}.lst-kix_lvvmpwix8sn0-8>li:before{content:"\0025a0   "}ul.lst-kix_g2z3zcyhlk66-7{list-style-type:none}ul.lst-kix_g2z3zcyhlk66-6{list-style-type:none}ul.lst-kix_g2z3zcyhlk66-8{list-style-type:none}.lst-kix_wu4ejk04ds8-1>li:before{content:"\0025cb   "}.lst-kix_wu4ejk04ds8-2>li:before{content:"\0025a0   "}.lst-kix_wu4ejk04ds8-3>li:before{content:"\0025cf   "}.lst-kix_wu4ejk04ds8-4>li:before{content:"\0025cb   "}.lst-kix_wu4ejk04ds8-7>li:before{content:"\0025cb   "}.lst-kix_wu4ejk04ds8-5>li:before{content:"\0025a0   "}.lst-kix_wu4ejk04ds8-6>li:before{content:"\0025cf   "}.lst-kix_wu4ejk04ds8-8>li:before{content:"\0025a0   "}ul.lst-kix_5izwj51keqm7-2{list-style-type:none}ul.lst-kix_5izwj51keqm7-1{list-style-type:none}ul.lst-kix_5izwj51keqm7-4{list-style-type:none}ul.lst-kix_5izwj51keqm7-3{list-style-type:none}ul.lst-kix_5izwj51keqm7-0{list-style-type:none}ul.lst-kix_z4hqe1y6t2ui-3{list-style-type:none}ul.lst-kix_z4hqe1y6t2ui-4{list-style-type:none}ul.lst-kix_z4hqe1y6t2ui-5{list-style-type:none}ul.lst-kix_z4hqe1y6t2ui-6{list-style-type:none}ul.lst-kix_z4hqe1y6t2ui-7{list-style-type:none}ul.lst-kix_z4hqe1y6t2ui-8{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_blglcosvosdw-6{list-style-type:none}ul.lst-kix_blglcosvosdw-7{list-style-type:none}ul.lst-kix_blglcosvosdw-4{list-style-type:none}ul.lst-kix_blglcosvosdw-5{list-style-type:none}ul.lst-kix_blglcosvosdw-8{list-style-type:none}ul.lst-kix_blglcosvosdw-2{list-style-type:none}ul.lst-kix_5izwj51keqm7-6{list-style-type:none}ul.lst-kix_blglcosvosdw-3{list-style-type:none}ul.lst-kix_5izwj51keqm7-5{list-style-type:none}ul.lst-kix_blglcosvosdw-0{list-style-type:none}ul.lst-kix_5izwj51keqm7-8{list-style-type:none}.lst-kix_wu4ejk04ds8-0>li:before{content:"\0025cf   "}ul.lst-kix_blglcosvosdw-1{list-style-type:none}ul.lst-kix_5izwj51keqm7-7{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c9{-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Arial";font-style:normal}.c14{margin-left:18pt;padding-top:12pt;text-indent:-18pt;padding-bottom:12pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c3{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c11{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:italic}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c16{padding-top:20pt;padding-bottom:6pt;line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c15{color:#999999;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c23{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c25{padding-top:12pt;padding-bottom:12pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c26{padding-top:12pt;padding-bottom:12pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c29{padding-top:12pt;padding-bottom:0pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c1{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c21{padding-top:12pt;padding-bottom:0pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c17{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:italic}.c30{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Times New Roman";font-style:normal}.c12{padding-top:0pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c24{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c22{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#0000ee;text-decoration:underline}.c20{padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c28{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c31{font-family:"Times New Roman";font-weight:400}.c4{color:#666666;font-size:16pt}.c32{padding:0;margin:0}.c8{color:inherit;text-decoration:inherit}.c10{font-size:12pt}.c27{height:11pt}.c19{font-style:italic}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c28 doc-content"><p class="c21 title" id="h.gym7jevfb7su"><span class="c10 c31">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 121.50px; height: 158.48px;"><img alt="" src="images/image1.png" style="width: 359.57px; height: 158.48px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c26 title" id="h.gym7jevfb7su-1"><span class="c6">Entrega # 3 - Clasificaci&oacute;n de im&aacute;genes</span></p><p class="c29 title" id="h.gym7jevfb7su-2"><span class="c10 c13">&nbsp;</span></p><p class="c26 title" id="h.gym7jevfb7su-3"><span class="c5">Docente:</span></p><p class="c30"><span class="c22"><a class="c8" href="mailto:jdospina@unal.edu.co">Juan David Ospina Arango</a></span></p><p class="c30 c27"><span class="c0"></span></p><p class="c26 title" id="h.gym7jevfb7su-4"><span class="c6">Fundamentos de Analitica</span></p><p class="c25 title" id="h.gym7jevfb7su-5"><span class="c13 c10">&nbsp;</span></p><p class="c26 title" id="h.gym7jevfb7su-6"><span class="c6">Equipo # 2:</span></p><p class="c14 title" id="h.gym7jevfb7su-7"><span class="c5">Cristian Jaramillo Herrera - CC:1036680346</span></p><p class="c14 title" id="h.6hucxm5pajt4"><span class="c5">Danilo Giraldo Lopez - CC:1007240582</span></p><p class="c14 title" id="h.kpj512eqz23b"><span class="c5">Juan Felipe Usuga Villegas - CC:1035922312 </span></p><p class="c14 title" id="h.ok7ub72k55yx"><span class="c5">Maria Camila Durango Mu&ntilde;oz - CC:1035234476 </span></p><p class="c30 c27"><span class="c5"><br><br><br></span></p><p class="c26 title" id="h.gym7jevfb7su-8"><span class="c10 c17">Universidad Nacional de Colombia, Sede Medell&iacute;n</span></p><p class="c26 title" id="h.gym7jevfb7su-9"><span class="c5">Departamento de la Computaci&oacute;n y la Decisi&oacute;n</span></p><p class="c26 title" id="h.gym7jevfb7su-10"><span class="c5">Ingenier&iacute;a de Sistemas e Inform&aacute;tica</span></p><p class="c26 title" id="h.gym7jevfb7su-11"><span class="c5">Facultad de Minas</span></p><p class="c21 title" id="h.8uqoq58pumx8"><span class="c10">22 de Noviembre de 2023</span><hr style="page-break-before:always;display:none;"></p><h1 class="c23" id="h.774sdowvd436"><span>&Iacute;ndice</span></h1><p class="c2"><span class="c0"></span></p><p class="c20"><span class="c9"><a class="c8" href="#h.774sdowvd436">&Iacute;ndice&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2</a></span></p><p class="c20"><span class="c9"><a class="c8" href="#h.gazne06omt0f">Resumen ejecutivo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3</a></span></p><p class="c20"><span class="c9"><a class="c8" href="#h.59laavfdvo0l">Introducci&oacute;n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3</a></span></p><p class="c20"><span class="c9"><a class="c8" href="#h.apaoim5o4tc1">Descripci&oacute;n del Modelo.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4</a></span></p><p class="c20"><span class="c9"><a class="c8" href="#h.w6w923dyzs9j">Exploraci&oacute;n de Datos&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4</a></span></p><p class="c20"><span class="c9"><a class="c8" href="#h.vhegr8khb3zb">Arquitectura del modelo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4</a></span></p><p class="c20"><span class="c9"><a class="c8" href="#h.a5fxq3vw9k3g">Entrenamiento del modelo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6</a></span></p><p class="c20"><span class="c9"><a class="c8" href="#h.zcphh47sitdv">Resultados iniciales&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6</a></span></p><p class="c20"><span class="c9"><a class="c8" href="#h.cd5brnwxmx6d">Almacenamiento del modelo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6</a></span></p><p class="c20"><span class="c9"><a class="c8" href="#h.8lvz9qyqepz6">An&aacute;lisis y Conclusiones&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6</a></span></p><hr style="page-break-before:always;display:none;"><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><h1 class="c16" id="h.gazne06omt0f"><span class="c18">Resumen ejecutivo</span></h1><p class="c12"><span class="c10 c19">El presente informe t&eacute;cnico describe el desarrollo e implementaci&oacute;n de un modelo de clasificaci&oacute;n para personas con gafas en distintas posiciones. El modelo se basa en una base de fotograf&iacute;as y utiliza t&eacute;cnicas de </span><span class="c10 c19">aprendizaje autom&aacute;tico</span><span class="c10 c19">&nbsp;para proporcionar una clasificaci&oacute;n confiable y repetible. Se obtiene como resultado un modelo que logre reconocer y separar las personas que porten o no unas gafas. </span></p><h1 class="c16" id="h.59laavfdvo0l"><span>Introducci&oacute;n</span></h1><p class="c12"><span class="c5">En la era contempor&aacute;nea, la visi&oacute;n por computadora y el aprendizaje profundo han emergido como campos de estudio fundamentales que han revolucionado la forma en que interactuamos con la tecnolog&iacute;a. La capacidad de las m&aacute;quinas para entender y procesar informaci&oacute;n visual es crucial en diversos sectores, desde la seguridad hasta la industria del entretenimiento.</span></p><p class="c12"><span class="c1">Objetivo del proyecto:</span></p><p class="c12"><span class="c5">El objetivo principal de este proyecto es desarrollar un modelo de clasificaci&oacute;n capaz de identificar con precisi&oacute;n si una persona en una imagen lleva gafas o no, independientemente de la posici&oacute;n de la cabeza y otros factores de variabilidad. La aplicaci&oacute;n de este modelo podr&iacute;a tener amplias implicaciones, desde sistemas de seguridad y vigilancia hasta aplicaciones de realidad aumentada. Para abordar esta tarea, se ha optado por utilizar redes neuronales, un enfoque dentro del aprendizaje profundo que ha demostrado ser altamente efectivo en problemas de visi&oacute;n por computadora.</span></p><p class="c12 c27"><span class="c5"></span></p><p class="c12"><span class="c5">A lo largo de este informe, se describir&aacute;n detalladamente los componentes clave del modelo, incluyendo la arquitectura de la red neuronal, el conjunto de datos utilizado para el entrenamiento y la evaluaci&oacute;n, as&iacute; como los resultados obtenidos durante el proceso de validaci&oacute;n.</span></p><h1 class="c16" id="h.apaoim5o4tc1"><span class="c18">Descripci&oacute;n del Modelo.</span></h1><p class="c7"><span class="c5">El modelo utiliza redes neuronales convolucionales (CNN) en Python, aprovechando las librer&iacute;as scikit-learn y TensorFlow. Se aplica un preprocesamiento cuidadoso, normalizando p&iacute;xeles y utilizando t&eacute;cnicas de aumento de datos con `ImageDataGenerator`.</span></p><p class="c2"><span class="c5"></span></p><p class="c7"><span class="c5">La arquitectura consta de capas convolucionales seguidas de capas de pooling para extraer y deducir caracter&iacute;sticas espaciales. Una capa completamente conectada integra estas caracter&iacute;sticas, seguida por una capa de Dropout para prevenir el sobreajuste. La capa de salida con activaci&oacute;n softmax realiza la clasificaci&oacute;n binaria. Aunque m&aacute;s adelante se entra en detalle de su arquitectura.</span></p><p class="c2"><span class="c5"></span></p><p class="c7"><span class="c5">Se utiliza la entrop&iacute;a cruzada binaria como funci&oacute;n de p&eacute;rdida y el optimizador Adam. El modelo se entrena y eval&uacute;a utilizando m&eacute;tricas como precisi&oacute;n, exhaustividad y F1-score. Las librer&iacute;as incluyen `LabelEncoder`, Keras y TensorFlow.</span></p><p class="c2"><span class="c5"></span></p><p class="c7"><span class="c5">La implementaci&oacute;n robusta y eficiente destaca el papel central de las CNN en resolver desaf&iacute;os complejos de visi&oacute;n por computadora.</span></p><p class="c2"><span class="c0"></span></p><h1 class="c16" id="h.w6w923dyzs9j"><span class="c18">Exploraci&oacute;n de Datos </span></h1><p class="c7"><span class="c1">Origen de los datos: </span></p><p class="c7"><span class="c10">Los datos empezaron c&oacute;mo una recopilaci&oacute;n de </span><span class="c10">640</span><span class="c10">&nbsp;im&aacute;genes de personas portando o no unos anteojos en diferentes &aacute;ngulos y posiciones. Se extrajeron los datos de la p&aacute;gina </span><span class="c10 c24"><a class="c8" href="https://www.google.com/url?q=https://archive.ics.uci.edu/dataset/124/cmu%2Bface%2Bimages&amp;sa=D&amp;source=editors&amp;ust=1700689795016179&amp;usg=AOvVaw1NvHBcGzWtJq9osoxkljB3">https://archive.ics.uci.edu/dataset/124/cmu+face+images</a></span><span class="c5">.</span></p><p class="c2"><span class="c5"></span></p><p class="c7"><span class="c1">Preprocesamiento de Datos:</span></p><p class="c12"><span class="c10">Antes de introducir las im&aacute;genes al modelo, se realiz&oacute; un cuidadoso preprocesamiento. La normalizaci&oacute;n de p&iacute;xeles y la aplicaci&oacute;n de t&eacute;cnicas de aumento de datos, mediante la biblioteca ImageDataGenerator de TensorFlow, jugaron un papel cr&iacute;tico para garantizar la robustez del modelo frente a variaciones en el conjunto de datos.</span></p><h1 class="c16" id="h.acx8rexhczg2"><span class="c18">Arquitectura del modelo</span></h1><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c7"><span class="c0">El dise&ntilde;o de la arquitectura del modelo se inicia con la capa de entrada, donde las im&aacute;genes en escala de grises se redimensionan a 64x60 p&iacute;xeles para facilitar el procesamiento. A continuaci&oacute;n, se implementan capas convolucionales para extraer caracter&iacute;sticas clave:</span></p><p class="c2"><span class="c0"></span></p><p class="c7"><span class="c15">1. Capa Convolucional (64 filtros, kernel size 3x3, ReLU):</span></p><p class="c7"><span class="c0">La primera capa convolucional utiliza 64 filtros con un tama&ntilde;o de kernel de 3x3 y la funci&oacute;n de activaci&oacute;n ReLU. Esta capa identifica patrones locales en la imagen.</span></p><p class="c2"><span class="c0"></span></p><p class="c7"><span class="c15">2. Capa de Max Pooling</span></p><p class="c7"><span class="c0">Posterior a cada capa convolucional, se aplica una capa de max pooling para reducir la dimensionalidad de la imagen y preservar las caracter&iacute;sticas m&aacute;s importantes.</span></p><p class="c2"><span class="c0"></span></p><p class="c7"><span class="c15">3. Capa Convolucional (128 filtros, kernel size 3x3, ReLU):</span></p><p class="c7"><span class="c0">Una segunda capa convolucional con 128 filtros y un tama&ntilde;o de kernel de 3x3 contin&uacute;a extrayendo caracter&iacute;sticas m&aacute;s complejas de la imagen.</span></p><p class="c2"><span class="c0"></span></p><p class="c7"><span class="c15">4. Capa de Max Pooling:</span></p><p class="c7"><span class="c0">Al igual que en la capa anterior, se aplica una capa de max pooling para consolidar las caracter&iacute;sticas extra&iacute;das.</span></p><p class="c2"><span class="c0"></span></p><p class="c7"><span class="c15">5. Capa Totalmente Conectada (256 neuronas, ReLU):</span></p><p class="c7"><span class="c0">Despu&eacute;s de las capas convolucionales, se introduce una capa totalmente conectada con 256 neuronas y activaci&oacute;n ReLU. Esta capa fusiona las caracter&iacute;sticas extra&iacute;das para una representaci&oacute;n m&aacute;s abstracta.</span></p><p class="c2"><span class="c0"></span></p><p class="c7"><span class="c15">6. Capa de Dropout:</span></p><p class="c7"><span class="c0">Con el objetivo de prevenir el sobreajuste, se implementa una capa de dropout despu&eacute;s de la capa totalmente conectada. Esto ayuda a regularizar el modelo al &quot;apagar&quot; aleatoriamente ciertas neuronas durante el entrenamiento.</span></p><p class="c2"><span class="c0"></span></p><p class="c7"><span class="c15">7. Capa de Salida (2 neuronas, softmax para clasificaci&oacute;n binaria):</span></p><p class="c7"><span class="c0">La capa de salida consta de dos neuronas con activaci&oacute;n softmax, adecuada para problemas de clasificaci&oacute;n binaria. Esta capa asigna probabilidades a las dos clases posibles (con gafas, sin gafas) para realizar la clasificaci&oacute;n final.</span></p><p class="c2"><span class="c0"></span></p><p class="c7"><span class="c0">Esta estructura estratificada de capas convolucionales, seguida de capas completamente conectadas, refleja la capacidad del modelo para aprender patrones a diferentes niveles de abstracci&oacute;n, desde caracter&iacute;sticas locales hasta representaciones m&aacute;s globales, proporcionando as&iacute; un enfoque integral para la clasificaci&oacute;n de im&aacute;genes en el contexto espec&iacute;fico de la detecci&oacute;n de gafas en personas.</span></p><p class="c2"><span class="c0"></span></p><h1 class="c16" id="h.y2945cns3c9u"><span class="c18">Entrenamiento del modelo</span></h1><p class="c7"><span class="c0">Para entrenar el modelo, se dividi&oacute; el conjunto de datos en dos partes: una para entrenamiento y otra para prueba (80% - 20%). Se us&oacute; la funci&oacute;n de p&eacute;rdida de entrop&iacute;a cruzada categ&oacute;rica, que mide la diferencia entre las probabilidades predichas por el modelo y las etiquetas reales. Tambi&eacute;n se us&oacute; el optimizador Adam, que adapta la tasa de aprendizaje seg&uacute;n el gradiente de la funci&oacute;n de p&eacute;rdida. El modelo se entren&oacute; durante 100 &eacute;pocas, con un tama&ntilde;o de lote de 64 im&aacute;genes.</span></p><h1 class="c16" id="h.39ffpj86pp3t"><span class="c18">Resultados iniciales</span></h1><p class="c7"><span class="c0">El modelo acert&oacute; en 9 de cada 10 im&aacute;genes. Este es un resultado bastante bueno, considerando que el modelo solo se entren&oacute; durante 100 &eacute;pocas.</span></p><h1 class="c16" id="h.p68hav89uvyd"><span>Almacenamiento del modelo</span></h1><p class="c7"><span class="c0">Para poder usar el modelo en el futuro, se guarda en un archivo con formato H5. Este formato permite almacenar la arquitectura, los pesos y la configuraci&oacute;n del modelo en un solo archivo. As&iacute;, se puede cargar el modelo f&aacute;cilmente y usarlo para hacer predicciones sobre nuevas im&aacute;genes.</span></p><h1 class="c16" id="h.a3pjvobdsrol"><span class="c18">An&aacute;lisis y Conclusiones</span></h1><p class="c12"><span class="c4">Resultados de la &Uacute;ltima &Eacute;poca (&Eacute;poca 100):</span></p><ul class="c32 lst-kix_blglcosvosdw-0 start"><li class="c3 li-bullet-0"><span class="c5">P&eacute;rdida de Entrenamiento: 0.3614</span></li><li class="c3 li-bullet-0"><span class="c5">Precisi&oacute;n de Entrenamiento: 83.23%</span></li><li class="c3 li-bullet-0"><span class="c5">P&eacute;rdida de Validaci&oacute;n: 0.2152</span></li><li class="c3 li-bullet-0"><span class="c5">Precisi&oacute;n de Validaci&oacute;n: 90.13%</span></li></ul><p class="c12"><span class="c4">Rendimiento General:</span><span class="c5">&nbsp;El modelo mostr&oacute; un buen rendimiento con una alta precisi&oacute;n de validaci&oacute;n. El modelo fue capaz de clasificar correctamente a las personas con y sin gafas en la mayor&iacute;a de los casos, m&aacute;s exactamente tuvo una precisi&oacute;n del 90.13%.</span></p><p class="c12"><span class="c4">Matriz de Confusi&oacute;n:</span><span class="c5">&nbsp;La matriz de confusi&oacute;n revel&oacute; que el modelo tuvo un rendimiento s&oacute;lido en ambas clases. La mayor parte de las predicciones se situaron en la diagonal principal, lo que indica una baja tasa de error.</span></p><p class="c12"><span class="c4">Tendencia de Entrenamiento:</span><span class="c5">&nbsp;La curva de p&eacute;rdida de entrenamiento mostr&oacute; una disminuci&oacute;n gradual a lo largo de las &eacute;pocas, lo que indica un aprendizaje efectivo. La curva de precisi&oacute;n de entrenamiento mostr&oacute; una mejora constante, alcanzando el 83.23%.</span></p><p class="c12"><span class="c4">Posible sobreajuste</span><span class="c5">: La diferencia entre la precisi&oacute;n de entrenamiento y la precisi&oacute;n de validaci&oacute;n podr&iacute;a indicar cierto sobreajuste, pero la brecha no es demasiado grande. Esto significa que el modelo no perdi&oacute; mucha capacidad de generalizaci&oacute;n al aprender de los datos de entrenamiento.</span></p><p class="c12"><span class="c4">Ajuste Adicional:</span><span class="c5">&nbsp;Se podr&iacute;a mejorar a&uacute;n m&aacute;s el rendimiento del modelo ajustando algunos hiperpar&aacute;metros como la tasa de aprendizaje, la tasa de abandono y la complejidad de la red. Tambi&eacute;n se podr&iacute;a probar con otros tipos de capas, funciones de activaci&oacute;n y optimizadores.</span></p><p class="c12"><span class="c4">Conclusi&oacute;n General:</span><span class="c5">&nbsp;El modelo logr&oacute; un nivel aceptable de precisi&oacute;n y generalizaci&oacute;n en la tarea de clasificaci&oacute;n de personas con y sin gafas. El modelo demostr&oacute; ser una herramienta &uacute;til para reconocer este tipo de rasgo facial.</span></p><p class="c12 c27"><span class="c5"></span></p></body></html>